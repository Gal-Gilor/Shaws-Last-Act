{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shaw's Last Act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GILOR\\Anaconda3\\envs\\torch\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as functional\n",
    "from ShawsDataset import ShawsDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from gensim.utils import tokenize\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text\n",
    "original_text_path = 'data/original_scripts.txt'\n",
    "with open(original_text_path, \"r\", encoding=\"utf8\") as line:\n",
    "    raw = line.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 95 unique characters in the text\n",
      "There are approximately 284240 words in the text\n",
      "There are approximately 49976 unique words in the text\n",
      "There are 35742 lines in the text\n",
      "On average, there are 7.952548822114039 words per line\n",
      "There are 9 different scripts in the text\n",
      "\n",
      "The text contains the scripts for the titles:\n",
      "  - Pygmalion\n",
      "  - Major Barbara\n",
      "  - Saint Joan\n",
      "  - Arms and the Man\n",
      "  - Man And Superma\n",
      "  - Mrs. Warren’s Profession\n",
      "  - Heartbreak House\n",
      "  - Caesar and Cleopatra\n",
      "  - You Never Can Tell\n"
     ]
    }
   ],
   "source": [
    "# text statatistics\n",
    "\n",
    "unique_chars = set(list(raw))\n",
    "print(f'There are {len(unique_chars)} unique characters in the text')\n",
    "\n",
    "n_words = len(raw.split(' '))\n",
    "print(f'There are approximately {n_words} words in the text')\n",
    "\n",
    "n_unique_words = len(set(raw.split(' ')))\n",
    "print(f'There are approximately {n_unique_words} unique words in the text')\n",
    "\n",
    "n_lines = len(raw.split('\\n'))\n",
    "print(f'There are {n_lines} lines in the text')\n",
    "\n",
    "print(f'On average, there are {n_words / n_lines} words per line')\n",
    "\n",
    "titles = re.findall('Title:.*\\n', raw)\n",
    "titles = [title.replace('\\n', '').replace('Title: ', '') for i, title in enumerate(titles)]\n",
    "print(f'There are {len(titles)} different scripts in the text\\n')\n",
    "print('The text contains the scripts for the titles:', *titles, sep='\\n  - ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!)—2]“[6”8ïæ5_\"?î½0’1-}'9.,3ôÉé;74{çè(&\n"
     ]
    }
   ],
   "source": [
    "# preprocess raw text\n",
    "\n",
    "def text_special_characters(text):\n",
    "\n",
    "    # identify all unique characters\n",
    "    unique_chars = list(set(list(text)))\n",
    "    \n",
    "    # merge the characters to a single string\n",
    "    unique_chars = ''.join(unique_chars)\n",
    "    \n",
    "    # remove letters and spaces \n",
    "    unique_chars = re.sub('[a-zA-Z\\s:]', '', unique_chars)\n",
    "    \n",
    "    puntuations = set(string.punctuations)\n",
    "    for char in unique_chars:\n",
    "        punctuations.add(char)\n",
    "    \n",
    "    return list(punctuations)\n",
    "\n",
    "\n",
    "# find unique characters that are not letters\n",
    "\n",
    "def special_characters_json(filepath):\n",
    "    with open(filepath, encoding='utf8') as line:\n",
    "        char2token = json.loads(line.read())\n",
    "        \n",
    "    token2char = {special: token for token, special in char2token.items()}\n",
    "    return (char2token, token2char)\n",
    "\n",
    "# tokenize special characters\n",
    "def tokenize_special_characters(text):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # load the special characters to tokenize\n",
    "    special2token, _ = special_characters_json('character_dictionary.json')\n",
    "\n",
    "    # replace special characters with the new tokens\n",
    "    for special, token in tokens_dict.items():\n",
    "        text = text.replace(special, f' {token} ')\n",
    "    \n",
    "    # replace multiple whitespaces with single whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "text = replace_special_characters(raw)\n",
    "\n",
    "tokenized_path = 'data/tokenized_scripts.txt'\n",
    "with open(tokenized_path, \"w\") as line:\n",
    "    line.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ShawsLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the PyTorch RNN Module\n",
    "        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)\n",
    "        :param output_size: The number of output dimensions of the neural network\n",
    "        :param embedding_dim: The size of embeddings, should you choose to use them        \n",
    "        :param hidden_dim: The size of the hidden layer outputs\n",
    "        :param dropout: dropout to add in between LSTM/GRU layers\n",
    "        \"\"\"\n",
    "        super(ShawsLSTM, self).__init__()\n",
    "        \n",
    "        # init hidden weights params\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # define the embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # define the LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=dropout, batch_first=True)\n",
    "\n",
    "        # define fully-connected layer\n",
    "        self.dense = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    \n",
    "    def forward(self, nn_input, hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        :param nn_input: The input to the neural network\n",
    "        :param hidden: The hidden state        \n",
    "        :return: Two Tensors, the output of the neural network and the latest hidden state\n",
    "        \"\"\"\n",
    "        # ensure embedding layer gets a LongTensor input\n",
    "        nn_input = nn_input.long()\n",
    "        \n",
    "        # get the batch size for reshaping\n",
    "        batch_size = nn_input.size(0)\n",
    "        \n",
    "        ## define forward pass\n",
    "        embed = self.embedding(nn_input)\n",
    "        output, state = self.lstm(embed, hidden)\n",
    "        \n",
    "        # stack LSTM\n",
    "        output = output.contiguous().view(-1, self.hidden_dim) \n",
    "        \n",
    "        # pass through last fully connected layer\n",
    "        output = self.dense(output)\n",
    "        \n",
    "        output = output.view(batch_size, -1, self.vocab_size)\n",
    "        output = output[:, -1] # save only the last output\n",
    "        \n",
    "        # return one batch of output word scores and the hidden state\n",
    "        return output, state   \n",
    "\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        '''\n",
    "        Initialize the hidden state of an LSTM\n",
    "        :param batch_size: The batch_size of the hidden state\n",
    "        :return: hidden state of dims (n_layers, batch_size, hidden_dim)\n",
    "        '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (torch.cuda.is_available()): #\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "SEQUENCE_LENGTH = 10\n",
    "dataset = ShawsDataset(tokenized_path, SEQUENCE_LENGTH)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence, target = next(iter(dataloader))\n",
    "sentence, target =  sentence.numpy().squeeze(), target.numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset\n",
    "# from collections import Counter\n",
    "# from gensim.utils import tokenize\n",
    "# from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# class Test(Dataset):\n",
    "#     '''\n",
    "#     Creates a custom PyTorch Dataset class\n",
    "#     args:\n",
    "#         filepath: string, path to text (UTF8)\n",
    "#         sequence_length: integer, sequence length\n",
    "#     '''\n",
    "#     def __init__(self, filepath, sequence_length):\n",
    "#         super(Test, self).__init__()\n",
    "#         self.filepath = filepath\n",
    "#         self.sequence_length = sequence_length\n",
    "        \n",
    "#         self.words = self.load_text()        \n",
    "#         self.tokens = list(tokenize(self.words, token_pattern='\\S+'))\n",
    "#         self.token_dict = Dictionary([self.tokens])\n",
    "        \n",
    "#         self.words_indexes = [self.token_dict.token2id[token] for token in self.tokens]\n",
    "    \n",
    "#     def load_text(self):\n",
    "#         with open(self.filepath, \"r\") as line:\n",
    "#             text = line.read()\n",
    "            \n",
    "#         # replace multiple whitespaces with single whitespace\n",
    "#         text = re.sub(r\"\\s+\", \" \", text)\n",
    "#         return text\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.words_indexes) - self.sequence_length\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         return (\n",
    "#             torch.tensor(self.words_indexes[index : index+self.sequence_length]),\n",
    "#             torch.tensor(self.words_indexes[index+self.sequence_length]),\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title : Pygmalion \n",
      " ACT I \n",
      " \n",
      " \n",
      " Covent\n"
     ]
    }
   ],
   "source": [
    "index = dataset.index_to_word\n",
    "new = []\n",
    "for word in sentence:\n",
    "    word = index[word]\n",
    "\n",
    "    \n",
    "    for token, char in token2special.items():\n",
    "        if word == token:\n",
    "            word = char\n",
    "            continue\n",
    "    \n",
    "    new.append(word)\n",
    "   \n",
    "\n",
    "    # Replace punctuation tokens\n",
    "    \n",
    "\n",
    "print(' '.join(new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title',\n",
       " ':',\n",
       " 'Pygmalion',\n",
       " 'ACT',\n",
       " 'I',\n",
       " 'Covent',\n",
       " 'Garden',\n",
       " 'at',\n",
       " '11',\n",
       " '.',\n",
       " '15',\n",
       " 'p',\n",
       " '.',\n",
       " 'm',\n",
       " '.',\n",
       " 'Torrents',\n",
       " 'of',\n",
       " 'heavy',\n",
       " 'summer',\n",
       " 'rain',\n",
       " '.',\n",
       " 'Cab',\n",
       " 'whistles',\n",
       " 'blowing',\n",
       " 'frantically',\n",
       " 'in',\n",
       " 'all',\n",
       " 'directions',\n",
       " '.',\n",
       " 'Pedestrians',\n",
       " 'running',\n",
       " 'for',\n",
       " 'shelter',\n",
       " 'into',\n",
       " 'the',\n",
       " 'market',\n",
       " 'and',\n",
       " 'under',\n",
       " 'the',\n",
       " 'po']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_sentence = raw[:200]\n",
    "work_sentence = re.sub(r\"\\s+\", \" \", work_sentence)\n",
    "\n",
    "import string\n",
    "punctuations=string.punctuation\n",
    "punctuations\n",
    "token_boundaries=[' ', '-']\n",
    "delimiter_token='<SPLIT>'\n",
    "\n",
    "for punctuation in punctuations:\n",
    "      work_sentence = work_sentence.replace(punctuation, \" \"+punctuation+\" \")\n",
    "\n",
    "        \n",
    "for delimiter in token_boundaries:\n",
    "    work_sentence = work_sentence.replace(delimiter, delimiter_token)\n",
    "    tokens = [x.strip() for x in work_sentence.split(delimiter_token) if x != '']\n",
    "\n",
    "    \n",
    "tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
