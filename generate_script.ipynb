{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shaw's Last Act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as functional\n",
    "from ShawsDataset import ShawsDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text\n",
    "original_text_path = 'data/original_scripts.txt'\n",
    "with open(original_text_path, \"r\", encoding=\"utf8\") as line:\n",
    "    raw = line.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 95 unique characters in the text\n",
      "There are approximately 284240 words in the text\n",
      "There are approximately 49976 unique words in the text\n",
      "There are 35742 lines in the text\n",
      "On average, there are 7.952548822114039 words per line\n",
      "There are 9 different scripts in the text\n",
      "\n",
      "The text contains the scripts for the titles:\n",
      "  - Pygmalion\n",
      "  - Major Barbara\n",
      "  - Saint Joan\n",
      "  - Arms and the Man\n",
      "  - Man And Superma\n",
      "  - Mrs. Warren’s Profession\n",
      "  - Heartbreak House\n",
      "  - Caesar and Cleopatra\n",
      "  - You Never Can Tell\n"
     ]
    }
   ],
   "source": [
    "# text statatistics\n",
    "\n",
    "unique_chars = set(list(raw))\n",
    "print(f'There are {len(unique_chars)} unique characters in the text')\n",
    "\n",
    "n_words = len(raw.split(' '))\n",
    "print(f'There are approximately {n_words} words in the text')\n",
    "\n",
    "n_unique_words = len(set(raw.split(' ')))\n",
    "print(f'There are approximately {n_unique_words} unique words in the text')\n",
    "\n",
    "n_lines = len(raw.split('\\n'))\n",
    "print(f'There are {n_lines} lines in the text')\n",
    "\n",
    "print(f'On average, there are {n_words / n_lines} words per line')\n",
    "\n",
    "titles = re.findall('Title:.*\\n', raw)\n",
    "titles = [title.replace('\\n', '').replace('Title: ', '') for i, title in enumerate(titles)]\n",
    "print(f'There are {len(titles)} different scripts in the text\\n')\n",
    "print('The text contains the scripts for the titles:', *titles, sep='\\n  - ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess raw text\n",
    "def text_special_characters(text):\n",
    "\n",
    "    # identify all unique characters\n",
    "    unique_chars = list(set(list(text)))\n",
    "    \n",
    "    # merge the characters to a single string\n",
    "    unique_chars = ''.join(unique_chars)\n",
    "    \n",
    "    # remove letters and spaces \n",
    "    unique_chars = re.sub('[a-zA-Z\\s+:]', '', unique_chars)\n",
    "    \n",
    "    return unique_chars\n",
    "\n",
    "def special_characters_json(filepath):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    with open(filepath, encoding='utf8') as line:\n",
    "        char2token = json.loads(line.read())\n",
    "        \n",
    "    token2char = {special: token for token, special in char2token.items()}\n",
    "    return (char2token, token2char)\n",
    "\n",
    "# tokenize special characters\n",
    "def tokenize_special_characters(text):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # load the special characters to tokenize\n",
    "    special2token, _ = special_characters_json('character_dictionary.json')\n",
    "\n",
    "    # replace special characters with the new tokens\n",
    "    for special, token in special2token.items():\n",
    "        text = text.replace(special, f' {token} ')\n",
    "    \n",
    "    # replace multiple whitespaces with single whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "text = tokenize_special_characters(raw)\n",
    "\n",
    "path_tokenized = 'data/tokenized_scripts.txt'\n",
    "with open(path_tokenized, \"w\") as line:\n",
    "    line.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ShawsLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
    "        '''\n",
    "        Initialize the PyTorch RNN Module\n",
    "        inputs:\n",
    "            vocab_size: integer, number of input dimensions (the size of the vocabulary)\n",
    "            output_size: integer, number of output dimensions (the size of the vocabulary)\n",
    "            embedding_dim: integer, word embedding dimensions       \n",
    "            hidden_dim: integer, number hidden layer output nodes\n",
    "            dropout: float, range between 0 and 1 to describe the chance of LSTM dropout layer (default= 0.5)\n",
    "        '''\n",
    "        super(ShawsLSTM, self).__init__()\n",
    "        \n",
    "        # init hidden weights params\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # define the embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # define the LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=dropout, batch_first=True)\n",
    "\n",
    "        # define fully-connected layer\n",
    "        self.dense = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    \n",
    "    def forward(self, nn_input, hidden):\n",
    "        '''\n",
    "        Returns the model output and the latest hidden state as Tensors\n",
    "        inputs:\n",
    "           nn_input: model inputs\n",
    "           hidden: the last hideen state        \n",
    "        '''\n",
    "        assert \"batch_size\" in dir(self), 'Initalize hidden weights first! -> init_hidden(batch_size)'\n",
    "        \n",
    "        # ensure embedding layer gets a LongTensor input\n",
    "        nn_input = nn_input.long()\n",
    "        \n",
    "        ## define forward pass\n",
    "        embed = self.embedding(nn_input)\n",
    "        output, state = self.lstm(embed, hidden)\n",
    "        \n",
    "        # stack LSTM\n",
    "        output = output.contiguous().view(-1, self.hidden_dim) \n",
    "        \n",
    "        # pass through last fully connected layer\n",
    "        output = self.dense(output)\n",
    "        \n",
    "        output = output.view(self.batch_size, -1, self.vocab_size)\n",
    "        output = output[:, -1] # save only the last output\n",
    "        \n",
    "        # return one batch of output word scores and the hidden state\n",
    "        return output, state   \n",
    "\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        '''\n",
    "        Initialize the hidden state of an LSTM in the shape (n_layers, batch_size, hidden_dim)\n",
    "        inputs:\n",
    "            batch_size: integer, the batch_size of the hidden state\n",
    "        \n",
    "        '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        if (torch.cuda.is_available()): #\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(rnn, optimizer, criterion, inputs, target, hidden, device):\n",
    "    '''\n",
    "    Completes the forward and backward propagation, and \n",
    "    returns the final hidden state and train loss\n",
    "        rnn: ShawsLSTM instance, PyTorch class that holds the model\n",
    "        optimizer: torch.optim, PyTorch optimizer\n",
    "        criterion: loss function class, PyTorch (or custom) loss function\n",
    "        inputs: torch Tensor, a batch of input to the neural network\n",
    "        target: torch Tensor, the target output for the batch of inputs\n",
    "    '''\n",
    "    \n",
    "    # move model to GPU, if available\n",
    "    rnn.to(device)\n",
    "    \n",
    "    # move data to GPU, if available\n",
    "    inputs, target = inputs.to(device), target.to(device)\n",
    "    \n",
    "    # dismember the hidden states to prevent backprop through entire training history\n",
    "    hidden = tuple([hid.data for hid in hidden])\n",
    "    \n",
    "    # zero accumulated gradients\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    # get the output and hidden state from the model\n",
    "    output, hidden = rnn(inputs, hidden)\n",
    "    \n",
    "    # calcualte the loss\n",
    "    loss = criterion(output.squeeze(), target.long())\n",
    "    \n",
    "    # perform backpropagation\n",
    "    loss.backward()\n",
    "    \n",
    "    # clip to prevent gradients from becoming too large before optimizating\n",
    "    nn.utils.clip_grad_value_(rnn.parameters(), 4)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # ensure everything is sent back to cpu\n",
    "    rnn.to(device)\n",
    "    inputs, target = inputs.to(device), target.to(device)\n",
    "    \n",
    "    # return the loss over a batch and the hidden state produced by our model\n",
    "    return loss.item(), hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "SEQUENCE_LENGTH = 10\n",
    "dataset = ShawsDataset(tokenized_path, SEQUENCE_LENGTH)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence, target = next(iter(dataloader))\n",
    "sentence, target =  sentence.numpy().squeeze(), target.numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2770,   17, 5863,    0, 1078,    5,    0,    0,    0, 3323],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset\n",
    "# from collections import Counter\n",
    "# from gensim.utils import tokenize\n",
    "# from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# class Test(Dataset):\n",
    "#     '''\n",
    "#     Creates a custom PyTorch Dataset class\n",
    "#     args:\n",
    "#         filepath: string, path to text (UTF8)\n",
    "#         sequence_length: integer, sequence length\n",
    "#     '''\n",
    "#     def __init__(self, filepath, sequence_length):\n",
    "#         super(Test, self).__init__()\n",
    "#         self.filepath = filepath\n",
    "#         self.sequence_length = sequence_length\n",
    "        \n",
    "#         self.words = self.load_text()        \n",
    "#         self.tokens = list(tokenize(self.words, token_pattern='\\S+'))\n",
    "#         self.token_dict = Dictionary([self.tokens])\n",
    "        \n",
    "#         self.words_indexes = [self.token_dict.token2id[token] for token in self.tokens]\n",
    "    \n",
    "#     def load_text(self):\n",
    "#         with open(self.filepath, \"r\") as line:\n",
    "#             text = line.read()\n",
    "            \n",
    "#         # replace multiple whitespaces with single whitespace\n",
    "#         text = re.sub(r\"\\s+\", \" \", text)\n",
    "#         return text\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.words_indexes) - self.sequence_length\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         return (\n",
    "#             torch.tensor(self.words_indexes[index : index+self.sequence_length]),\n",
    "#             torch.tensor(self.words_indexes[index+self.sequence_length]),\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9ddeeca9dc95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_to_word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "index = dataset.index_to_word\n",
    "new = []\n",
    "for word in sentence:\n",
    "    word = index[word]\n",
    "\n",
    "    \n",
    "    for token, char in token2special.items():\n",
    "        if word == token:\n",
    "            word = char\n",
    "            continue\n",
    "    \n",
    "    new.append(word)\n",
    "   \n",
    "\n",
    "    # Replace punctuation tokens\n",
    "    \n",
    "\n",
    "print(' '.join(new).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title',\n",
       " ':',\n",
       " 'Pygmalion',\n",
       " 'ACT',\n",
       " 'I',\n",
       " 'Covent',\n",
       " 'Garden',\n",
       " 'at',\n",
       " '11',\n",
       " '.',\n",
       " '15',\n",
       " 'p',\n",
       " '.',\n",
       " 'm',\n",
       " '.',\n",
       " 'Torrents',\n",
       " 'of',\n",
       " 'heavy',\n",
       " 'summer',\n",
       " 'rain',\n",
       " '.',\n",
       " 'Cab',\n",
       " 'whistles',\n",
       " 'blowing',\n",
       " 'frantically',\n",
       " 'in',\n",
       " 'all',\n",
       " 'directions',\n",
       " '.',\n",
       " 'Pedestrians',\n",
       " 'running',\n",
       " 'for',\n",
       " 'shelter',\n",
       " 'into',\n",
       " 'the',\n",
       " 'market',\n",
       " 'and',\n",
       " 'under',\n",
       " 'the',\n",
       " 'po']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_sentence = raw[:200]\n",
    "work_sentence = re.sub(r\"\\s+\", \" \", work_sentence)\n",
    "\n",
    "import string\n",
    "punctuations=string.punctuation\n",
    "punctuations\n",
    "token_boundaries=[' ', '-']\n",
    "delimiter_token='<SPLIT>'\n",
    "\n",
    "for punctuation in punctuations:\n",
    "      work_sentence = work_sentence.replace(punctuation, \" \"+punctuation+\" \")\n",
    "\n",
    "        \n",
    "for delimiter in token_boundaries:\n",
    "    work_sentence = work_sentence.replace(delimiter, delimiter_token)\n",
    "    tokens = [x.strip() for x in work_sentence.split(delimiter_token) if x != '']\n",
    "\n",
    "    \n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '[',\n",
       " 1: '5',\n",
       " 2: ']',\n",
       " 3: '”',\n",
       " 4: '’',\n",
       " 5: 'ô',\n",
       " 6: 'ç',\n",
       " 7: '6',\n",
       " 8: '½',\n",
       " 9: '!',\n",
       " 10: '{',\n",
       " 11: 'î',\n",
       " 12: 'É',\n",
       " 13: ';',\n",
       " 14: '“',\n",
       " 15: '9',\n",
       " 16: '4',\n",
       " 17: ',',\n",
       " 18: 'è',\n",
       " 19: '\"',\n",
       " 20: '2',\n",
       " 21: '}',\n",
       " 22: '0',\n",
       " 23: 'æ',\n",
       " 24: '&',\n",
       " 25: '_',\n",
       " 26: '3',\n",
       " 27: '(',\n",
       " 28: '8',\n",
       " 29: '?',\n",
       " 30: '.',\n",
       " 31: '—',\n",
       " 32: 'é',\n",
       " 33: '7',\n",
       " 34: \"'\",\n",
       " 35: ')',\n",
       " 36: '1',\n",
       " 37: '-',\n",
       " 38: 'ï'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_special_characters(text):\n",
    "\n",
    "    # identify all unique characters\n",
    "    unique_chars = list(set(list(text)))\n",
    "    \n",
    "    # merge the characters to a single string\n",
    "    unique_chars = ''.join(unique_chars)\n",
    "    \n",
    "    # remove letters and spaces \n",
    "    unique_chars = re.sub('[a-zA-Z\\s:]', '', unique_chars)\n",
    "    \n",
    "    return unique_chars\n",
    "\n",
    "a = text_special_characters(raw)\n",
    "\n",
    "dict(enumerate(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classic():\n",
    "    def __init__(self):\n",
    "        self.mine = 0\n",
    "        \n",
    "    def forward(self):\n",
    "        assert \"batch_size\" in dir(self), 'Initalize hidden weights first!'\n",
    "        return 1\n",
    "    \n",
    "    def inital(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Classic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.inital(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Initalize hidden weights first!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-4593cff02db5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-42-aa886f53fec5>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[1;34m\"batch_size\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Initalize hidden weights first!'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Initalize hidden weights first!"
     ]
    }
   ],
   "source": [
    "c.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
