{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shaw's Last Act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as functional\n",
    "from ShawsDataset import ShawsDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text\n",
    "original_text_path = 'data/original_scripts.txt'\n",
    "with open(original_text_path, \"r\", encoding=\"utf8\") as line:\n",
    "    raw = line.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 95 unique characters in the text\n",
      "There are approximately 284240 words in the text\n",
      "There are approximately 49976 unique words in the text\n",
      "There are 35742 lines in the text\n",
      "On average, there are 7.952548822114039 words per line\n",
      "There are 9 different scripts in the text\n",
      "\n",
      "The text contains the scripts for the titles:\n",
      "  - Pygmalion\n",
      "  - Major Barbara\n",
      "  - Saint Joan\n",
      "  - Arms and the Man\n",
      "  - Man And Superma\n",
      "  - Mrs. Warrenâ€™s Profession\n",
      "  - Heartbreak House\n",
      "  - Caesar and Cleopatra\n",
      "  - You Never Can Tell\n"
     ]
    }
   ],
   "source": [
    "# text statatistics\n",
    "\n",
    "unique_chars = set(list(raw))\n",
    "print(f'There are {len(unique_chars)} unique characters in the text')\n",
    "\n",
    "n_words = len(raw.split(' '))\n",
    "print(f'There are approximately {n_words} words in the text')\n",
    "\n",
    "n_unique_words = len(set(raw.split(' ')))\n",
    "print(f'There are approximately {n_unique_words} unique words in the text')\n",
    "\n",
    "n_lines = len(raw.split('\\n'))\n",
    "print(f'There are {n_lines} lines in the text')\n",
    "\n",
    "print(f'On average, there are {n_words / n_lines} words per line')\n",
    "\n",
    "titles = re.findall('Title:.*\\n', raw)\n",
    "titles = [title.replace('\\n', '').replace('Title: ', '') for i, title in enumerate(titles)]\n",
    "print(f'There are {len(titles)} different scripts in the text\\n')\n",
    "print('The text contains the scripts for the titles:', *titles, sep='\\n  - ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting property name enclosed in double quotes: line 29 column 1 (char 690)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-fb2c137459d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize_special_characters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[0mpath_tokenized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'data/tokenized_scripts.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-fb2c137459d7>\u001b[0m in \u001b[0;36mtokenize_special_characters\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     29\u001b[0m     '''\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# load the special characters to tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mspecial2token\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspecial_characters_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'character_dictionary.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;31m# replace special characters with the new tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-fb2c137459d7>\u001b[0m in \u001b[0;36mspecial_characters_json\u001b[1;34m(filepath)\u001b[0m\n\u001b[0;32m     18\u001b[0m     '''\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mchar2token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mtoken2char\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mspecial\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspecial\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchar2token\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 348\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \"\"\"\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    351\u001b[0m         \"\"\"\n\u001b[0;32m    352\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 29 column 1 (char 690)"
     ]
    }
   ],
   "source": [
    "# preprocess raw text\n",
    "def text_special_characters(text):\n",
    "\n",
    "    # identify all unique characters\n",
    "    unique_chars = list(set(list(text)))\n",
    "    \n",
    "    # merge the characters to a single string\n",
    "    unique_chars = ''.join(unique_chars)\n",
    "    \n",
    "    # remove letters and spaces \n",
    "    unique_chars = re.sub('[a-zA-Z\\s+:]', '', unique_chars)\n",
    "    \n",
    "    return unique_chars\n",
    "\n",
    "def special_characters_json(filepath):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    with open(filepath, encoding='utf8') as line:\n",
    "        char2token = json.loads(line.read())\n",
    "        \n",
    "    token2char = {special: token for token, special in char2token.items()}\n",
    "    return (char2token, token2char)\n",
    "\n",
    "# tokenize special characters\n",
    "def tokenize_special_characters(text):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # load the special characters to tokenize\n",
    "    special2token, _ = special_characters_json('character_dictionary.json')\n",
    "\n",
    "    # replace special characters with the new tokens\n",
    "    for special, token in special2token.items():\n",
    "        text = text.replace(special, f' {token} ')\n",
    "    \n",
    "    # replace multiple whitespaces with single whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "text = tokenize_special_characters(raw)\n",
    "\n",
    "path_tokenized = 'data/tokenized_scripts.txt'\n",
    "with open(path_tokenized, \"w\") as line:\n",
    "    line.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ShawsLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
    "        '''\n",
    "        Initialize the PyTorch RNN Module\n",
    "        inputs:\n",
    "            vocab_size: integer, number of input dimensions (the size of the vocabulary)\n",
    "            output_size: integer, number of output dimensions (the size of the vocabulary)\n",
    "            embedding_dim: integer, word embedding dimensions       \n",
    "            hidden_dim: integer, number hidden layer output nodes\n",
    "            dropout: float, range between 0 and 1 to describe the chance of LSTM dropout layer (default= 0.5)\n",
    "        '''\n",
    "        super(ShawsLSTM, self).__init__()\n",
    "        \n",
    "        # init hidden weights params\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # define the embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # define the LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=dropout, batch_first=True)\n",
    "\n",
    "        # define fully-connected layer\n",
    "        self.dense = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    \n",
    "    def forward(self, nn_input, hidden):\n",
    "        '''\n",
    "        Returns the model output and the latest hidden state as Tensors\n",
    "        inputs:\n",
    "           nn_input: model inputs\n",
    "           hidden: the last hideen state        \n",
    "        '''\n",
    "        assert hasattr(self, \"batch_size\"), 'Initalize hidden weights first! -> init_hidden(batch_size)'\n",
    "        \n",
    "        # ensure embedding layer gets a LongTensor input\n",
    "        nn_input = nn_input.long()\n",
    "        \n",
    "        ## define forward pass\n",
    "        embed = self.embedding(nn_input)\n",
    "        output, state = self.lstm(embed, hidden)\n",
    "        \n",
    "        # stack LSTM\n",
    "        output = output.contiguous().view(-1, self.hidden_dim) \n",
    "        \n",
    "        # pass through last fully connected layer\n",
    "        output = self.dense(output)\n",
    "        \n",
    "        output = output.view(self.batch_size, -1, self.vocab_size)\n",
    "        output = output[:, -1] # save only the last output\n",
    "        \n",
    "        # return one batch of output word scores and the hidden state\n",
    "        return output, state   \n",
    "\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        '''\n",
    "        Initialize the hidden state of an LSTM in the shape (n_layers, batch_size, hidden_dim)\n",
    "        inputs:\n",
    "            batch_size: integer, the batch_size of the hidden state\n",
    "        \n",
    "        '''\n",
    "       \n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # reshape, zero, and move to device\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(rnn, optimizer, criterion, inputs, target, hidden, device):\n",
    "    '''\n",
    "    Completes the forward and backward propagation, and \n",
    "    returns the final hidden state and train loss\n",
    "        rnn: ShawsLSTM instance, PyTorch class\n",
    "        optimizer: torch.optim, PyTorch optimizer\n",
    "        criterion: loss function class, PyTorch (or custom) loss function\n",
    "        inputs: torch Tensor, a batch of input to the neural network\n",
    "        target: torch Tensor, the target output for the batch of inputs\n",
    "    '''\n",
    "    \n",
    "    # move model to GPU, if available\n",
    "    rnn.to(device)\n",
    "    \n",
    "    # move data to GPU, if available\n",
    "    inputs, target = inputs.to(device), target.to(device)\n",
    "    \n",
    "    # dismember the hidden states to prevent backprop through entire training history\n",
    "    hidden = tuple([hid.data for hid in hidden])\n",
    "    \n",
    "    # zero accumulated gradients\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    # get the output and hidden state from the model\n",
    "    output, hidden = rnn(inputs, hidden)\n",
    "    \n",
    "    # calcualte the loss\n",
    "    loss = criterion(output.squeeze(), target.long())\n",
    "    \n",
    "    # perform backpropagation\n",
    "    loss.backward()\n",
    "    \n",
    "    # clip to prevent gradients from becoming too large before optimizating\n",
    "    nn.utils.clip_grad_value_(rnn.parameters(), 4)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # ensure everything is sent back to cpu\n",
    "    rnn.to(device)\n",
    "    inputs, target = inputs.to(device), target.to(device)\n",
    "    \n",
    "    # return the loss over a batch and the hidden state produced by our model\n",
    "    return loss.item(), hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    batch_losses = []\n",
    "    with rnn.train():\n",
    "        print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "        for epoch_i in tqdm(range(1, n_epochs + 1)):\n",
    "\n",
    "\n",
    "            # initialize hidden state\n",
    "            hidden = rnn.init_hidden(batch_size)\n",
    "\n",
    "            for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "\n",
    "                # make sure you iterate over completely full batches, only\n",
    "                n_batches = len(train_loader.dataset)//batch_size\n",
    "                if(batch_i > n_batches):\n",
    "                    break\n",
    "\n",
    "                # forward, back prop\n",
    "                loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n",
    "                # record loss\n",
    "                batch_losses.append(loss)\n",
    "\n",
    "                # printing loss stats\n",
    "                if batch_i % show_every_n_batches == 0:\n",
    "                    print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n",
    "                        epoch_i, n_epochs, np.average(batch_losses)))\n",
    "                    batch_losses = []\n",
    "\n",
    "    # returns a trained rnn\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "SEQUENCE_LENGTH = 10\n",
    "dataset = ShawsDataset(tokenized_path, SEQUENCE_LENGTH)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence, target = next(iter(dataloader))\n",
    "sentence, target =  sentence.numpy().squeeze(), target.numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset\n",
    "# from collections import Counter\n",
    "# from gensim.utils import tokenize\n",
    "# from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# class Test(Dataset):\n",
    "#     '''\n",
    "#     Creates a custom PyTorch Dataset class\n",
    "#     args:\n",
    "#         filepath: string, path to text (UTF8)\n",
    "#         sequence_length: integer, sequence length\n",
    "#     '''\n",
    "#     def __init__(self, filepath, sequence_length):\n",
    "#         super(Test, self).__init__()\n",
    "#         self.filepath = filepath\n",
    "#         self.sequence_length = sequence_length\n",
    "        \n",
    "#         self.words = self.load_text()        \n",
    "#         self.tokens = list(tokenize(self.words, token_pattern='\\S+'))\n",
    "#         self.token_dict = Dictionary([self.tokens])\n",
    "        \n",
    "#         self.words_indexes = [self.token_dict.token2id[token] for token in self.tokens]\n",
    "    \n",
    "#     def load_text(self):\n",
    "#         with open(self.filepath, \"r\") as line:\n",
    "#             text = line.read()\n",
    "            \n",
    "#         # replace multiple whitespaces with single whitespace\n",
    "#         text = re.sub(r\"\\s+\", \" \", text)\n",
    "#         return text\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.words_indexes) - self.sequence_length\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         return (\n",
    "#             torch.tensor(self.words_indexes[index : index+self.sequence_length]),\n",
    "#             torch.tensor(self.words_indexes[index+self.sequence_length]),\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = dataset.index_to_word\n",
    "new = []\n",
    "for word in sentence:\n",
    "    word = index[word]\n",
    "\n",
    "    \n",
    "    for token, char in token2special.items():\n",
    "        if word == token:\n",
    "            word = char\n",
    "            continue\n",
    "    \n",
    "    new.append(word)\n",
    "   \n",
    "\n",
    "    # Replace punctuation tokens\n",
    "    \n",
    "\n",
    "print(' '.join(new).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_sentence = raw[:200]\n",
    "work_sentence = re.sub(r\"\\s+\", \" \", work_sentence)\n",
    "\n",
    "import string\n",
    "punctuations=string.punctuation\n",
    "punctuations\n",
    "token_boundaries=[' ', '-']\n",
    "delimiter_token='<SPLIT>'\n",
    "\n",
    "for punctuation in punctuations:\n",
    "      work_sentence = work_sentence.replace(punctuation, \" \"+punctuation+\" \")\n",
    "\n",
    "        \n",
    "for delimiter in token_boundaries:\n",
    "    work_sentence = work_sentence.replace(delimiter, delimiter_token)\n",
    "    tokens = [x.strip() for x in work_sentence.split(delimiter_token) if x != '']\n",
    "\n",
    "    \n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_special_characters(text):\n",
    "\n",
    "    # identify all unique characters\n",
    "    unique_chars = list(set(list(text)))\n",
    "    \n",
    "    # merge the characters to a single string\n",
    "    unique_chars = ''.join(unique_chars)\n",
    "    \n",
    "    # remove letters and spaces \n",
    "    unique_chars = re.sub('[a-zA-Z\\s:]', '', unique_chars)\n",
    "    \n",
    "    return unique_chars\n",
    "\n",
    "a = text_special_characters(raw)\n",
    "\n",
    "dict(enumerate(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classic():\n",
    "    def __init__(self):\n",
    "        self.mine = 0\n",
    "        \n",
    "    def forward(self):\n",
    "        assert hasattr(self, \"batch_size\"), 'Initalize hidden weights first!'\n",
    "        return 1\n",
    "    \n",
    "    def inital(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Classic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.inital(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'cpu' == torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range(1), ascii=True):\n",
    "    str(i)\n",
    "    for n in range(100):\n",
    "        float(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(tqdm)\n",
    "#help(tqdm.in_place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
